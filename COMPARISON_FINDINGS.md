# STGen Protocol Comparison: Key Findings

## Summary of Results

### Baseline Test (No Network Impairment)
| Protocol | Messages Sent | Loss | Avg Latency | Status |
|----------|--------------|------|-------------|--------|
| MQTT     | 15,000       | 0.00% | 0.31 ms    | âœ… Valid |
| CoAP     | 15,000       | 0.00% | 0.75 ms    | âœ… Valid |
| my_udp   | 15,815       | 0.00% | 0.05 ms    | âœ… Valid |

**Conclusion**: All protocols handle the target load (100 clients @ 10Hz) correctly on localhost.

---

### Network-Impaired Test (0.5% loss, 10ms latency)
| Protocol | Messages Sent | Expected | Achievement | Avg Latency |
|----------|--------------|----------|-------------|-------------|
| MQTT     | 430          | 15,000   | 2.9%       | 35.18 ms    |
| CoAP     | 335          | 15,000   | 2.2%       | 44.74 ms    |
| my_udp   | 15,738       | 15,000   | 104.9%     | 9.99 ms     |

**Conclusion**: Active Python protocols (MQTT/CoAP) are severely throttled by network delay, while passive C binaries (my_udp) maintain full throughput.

---

## Root Cause Analysis

### Problem: Synchronous Orchestrator Bottleneck

The STGen orchestrator implements **active mode** as a serialized loop:

```python
for cid, payload, interval in stream:
    t0 = time.perf_counter()
    
    # âŒ BLOCKING CALL - waits for network RTT
    ok, t_srv = self.protocol.send_data(cid, payload)
    
    # Try to compensate for drift
    next_wake_time += interval
    sleep_time = next_wake_time - time.perf_counter()
    
    if sleep_time > 0:
        time.sleep(sleep_time)
```

**With Network Delay:**
- Target interval: 1ms (1000 messages/second for 100 clients @ 10Hz)
- Actual `send_data()` time: 10-20ms (network RTT)
- **Result**: Orchestrator can only send ~50-100 msg/s instead of 1000 msg/s

This is an **architectural limitation**, not a protocol deficiency.

---

## Why my_udp Works

The passive mode C binaries use **true parallelism**:

```c
// Each client is a separate process
for (int i = 0; i < 100; i++) {
    pid_t pid = fork();
    if (pid == 0) {
        // Child process sends independently
        while (duration--) {
            send_udp_packet();
            usleep(100000);  // 100ms = 10Hz
        }
        exit(0);
    }
}
```

**Result**: All 100 processes send concurrently, unaffected by network delay.

---

## Protocol Comparison: Realistic Takeaways

### Performance Characteristics

| Protocol | Architecture | Throughput (Impaired) | Latency | Reliability |
|----------|-------------|----------------------|---------|-------------|
| **my_udp** | Multi-process C | âœ… Excellent (15k msg) | âœ… 10ms | âš ï¸ No ACK (fire-and-forget) |
| **MQTT** | Single-thread Python | âŒ Poor (430 msg) | âš ï¸ 35ms | âœ… QoS with ACK |
| **CoAP** | Single-thread Python | âŒ Poor (335 msg) | âŒ 45ms | âœ… Confirmable messages |

### Context-Aware Recommendations

#### For High-Throughput IoT (>1000 msg/s)
- **Use**: Custom C/Rust implementations with multi-threading
- **Avoid**: Single-threaded Python orchestrators
- **Alternative**: Implement async/await in STGen orchestrator

#### For Reliable Data Delivery
- **Use**: MQTT with QoS=1 or CoAP with Confirmable messages
- **Avoid**: Raw UDP (unless application-level ACKs are implemented)

#### For Real-Time Applications (<50ms latency)
- **Use**: UDP-based protocols (CoAP, custom)
- **Avoid**: MQTT (TCP overhead + broker delay)

#### For Battery-Constrained Devices
- **Use**: CoAP (UDP, smaller packets)
- **Avoid**: MQTT (persistent TCP connections)

---

## Localhost Testing Limitations

### What Localhost Hides

1. **Packet Loss Recovery**: 
   - Localhost: 0% loss (perfect virtual network)
   - Real WiFi: 1-5% loss â†’ MQTT/CoAP automatically retry, UDP drops silently

2. **Congestion Behavior**:
   - Localhost: Infinite bandwidth
   - Real network: Protocols compete for spectrum

3. **True Latency**:
   - Localhost + 10ms emulation: ~10-45ms
   - Real LTE: 50-200ms
   - Real LoRaWAN: 1-10 seconds

### Recommended Test Methodology

For **accurate** protocol comparisons:

1. **Multi-Process Architecture**: Modify STGen to fork parallel client processes
2. **Real Hardware**: Test on actual Raspberry Pi / ESP32 devices
3. **Real Networks**: Use WiFi/LTE, not tc emulation
4. **Measure Actual Reliability**: 
   - Track sequence numbers for loss detection
   - Measure end-to-end delivery confirmation time

---

## Fixes Applied

### 1. Reduced Network Impairment Defaults
```python
# Before: 50ms latency, 2% loss (too harsh for localhost)
# After:  10ms latency, 0.5% loss (gentler)
```

### 2. Suppressed aiocoap Shutdown Errors
```python
# Known race condition in aiocoap library
except AttributeError as e:
    if "'NoneType' object has no attribute 'values'" in str(e):
        pass  # Ignore cleanup bug
```

### 3. Increased CoAP Timeout
```python
# Before: 10s timeout
# After:  30s timeout (handles network-impaired conditions)
```

---

## Conclusion

**The comparison is now fair in terms of message count** (~15,000 each) under ideal conditions. However, **network impairment reveals an architectural limitation** in STGen's active mode that unfairly penalizes Python-based protocols.

**Recommendation**: For production IoT systems requiring high throughput under network stress, implement protocols as **independent concurrent processes** rather than orchestrated by a central serialized loop.

---

### Next Steps

1. âœ… **Baseline comparison works** (all protocols send 15k messages)
2. âš ï¸ **Network-impaired comparison reveals orchestrator bottleneck**
3. ðŸ”„ **Future work**: Refactor orchestrator to use asyncio or multi-processing for active protocols
4. ðŸ“Š **Use results appropriately**: 
   - Localhost baseline: Valid for protocol overhead comparison
   - Network-impaired: Shows architectural limits, not protocol limits
